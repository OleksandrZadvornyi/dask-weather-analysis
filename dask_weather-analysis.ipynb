{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8000b2-6540-425c-b7db-903bc6862739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as db\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask_cloudprovider.gcp import GCPCluster\n",
    "from google.auth import default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f580989-c7c6-4cdf-9715-e65413c9bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Step 1: Start a Local Cluster\n",
    "local_cluster = LocalCluster(\n",
    "    n_workers=int(0.9 * mp.cpu_count()),  # Use ~90% of available CPUs\n",
    "    processes=True,\n",
    "    threads_per_worker=1,\n",
    "    memory_limit=\"2GB\",\n",
    ")\n",
    "client = Client(local_cluster)\n",
    "print(f\"Dashboard available at: {client.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132c8b46-acaa-45c7-b7e7-e58bed54ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Step 2: Define Data Processing Code\n",
    "\n",
    "# Get a list of all station files\n",
    "data_path = \"D:/daily-summaries-latest-test\"  \n",
    "station_files = glob.glob(os.path.join(data_path, \"*\"))\n",
    "\n",
    "print(f\"Found {len(station_files)} station files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96fed12-78bf-4ad2-beab-3f8e26658529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single station file\n",
    "def process_station(filename):\n",
    "    # Extract station ID from filename\n",
    "    station_id = os.path.basename(filename)\n",
    "    \n",
    "    # Read the CSV file with the correct column structure\n",
    "    try:\n",
    "        df = pd.read_csv(filename, \n",
    "                        parse_dates=['DATE'],\n",
    "                        na_values=['', 'NA', 'NULL'])\n",
    "        \n",
    "        # Make sure required columns exist\n",
    "        required_cols = ['DATE', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'NAME', \n",
    "                         'PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN']\n",
    "        \n",
    "        # Check if all required columns exist, if not return limited data\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"Warning: File {station_id} is missing columns: {missing_cols}\")\n",
    "        \n",
    "        # Convert numeric columns to float, handling any non-numeric values\n",
    "        numeric_cols = ['LATITUDE', 'LONGITUDE', 'ELEVATION', 'PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN']\n",
    "        for col in numeric_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Skip attribute columns for calculations\n",
    "        # Perform some analysis on this station's data\n",
    "        monthly_data = None\n",
    "        if 'DATE' in df.columns:\n",
    "            # Calculate monthly averages for temperature and precipitation\n",
    "            agg_dict = {}\n",
    "            if 'TMAX' in df.columns:\n",
    "                agg_dict['TMAX'] = 'mean'\n",
    "            if 'TMIN' in df.columns:\n",
    "                agg_dict['TMIN'] = 'mean'\n",
    "            if 'PRCP' in df.columns:\n",
    "                agg_dict['PRCP'] = 'sum'\n",
    "            if 'SNOW' in df.columns:\n",
    "                agg_dict['SNOW'] = 'sum'\n",
    "            if 'SNWD' in df.columns:\n",
    "                agg_dict['SNWD'] = 'mean'  # Average snow depth\n",
    "            \n",
    "            if agg_dict:  # Only perform groupby if we have columns to aggregate\n",
    "                monthly_data = df.groupby(pd.Grouper(key='DATE', freq='M')).agg(agg_dict).reset_index()\n",
    "        \n",
    "        # Calculate station metadata\n",
    "        metadata = {\n",
    "            'station_id': station_id,\n",
    "            'name': df['NAME'].iloc[0] if 'NAME' in df.columns else 'Unknown',\n",
    "            'latitude': df['LATITUDE'].iloc[0] if 'LATITUDE' in df.columns else None,\n",
    "            'longitude': df['LONGITUDE'].iloc[0] if 'LONGITUDE' in df.columns else None,\n",
    "            'elevation': df['ELEVATION'].iloc[0] if 'ELEVATION' in df.columns else None,\n",
    "            'record_length': len(df),\n",
    "        }\n",
    "        \n",
    "        # Date range calculation\n",
    "        if 'DATE' in df.columns:\n",
    "            metadata['date_range'] = (df['DATE'].min(), df['DATE'].max())\n",
    "        \n",
    "        # Climate statistics\n",
    "        if 'TMAX' in df.columns and df['TMAX'].notna().any():\n",
    "            metadata['avg_tmax'] = df['TMAX'].mean()\n",
    "            metadata['max_tmax'] = df['TMAX'].max()\n",
    "        \n",
    "        if 'TMIN' in df.columns and df['TMIN'].notna().any():\n",
    "            metadata['avg_tmin'] = df['TMIN'].mean()\n",
    "            metadata['min_tmin'] = df['TMIN'].min()\n",
    "        \n",
    "        if 'PRCP' in df.columns and df['PRCP'].notna().any():\n",
    "            metadata['total_precip'] = df['PRCP'].sum()\n",
    "            metadata['max_precip'] = df['PRCP'].max()\n",
    "            metadata['precip_days'] = (df['PRCP'] > 0).sum()\n",
    "        \n",
    "        if 'SNOW' in df.columns and df['SNOW'].notna().any():\n",
    "            metadata['total_snow'] = df['SNOW'].sum()\n",
    "            metadata['max_snow'] = df['SNOW'].max()\n",
    "            metadata['snow_days'] = (df['SNOW'] > 0).sum()\n",
    "        \n",
    "        # Add monthly data if available\n",
    "        if monthly_data is not None:\n",
    "            metadata['monthly_data'] = monthly_data\n",
    "            \n",
    "        return metadata\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {station_id}: {str(e)}\")\n",
    "        return {\n",
    "            'station_id': station_id,\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0215d3cd-dd15-44b3-9e6d-2d907a3c7fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Step 3: Process Data Using Local Workers\n",
    "\n",
    "# Create a bag from our filenames\n",
    "files_bag = db.from_sequence(station_files)\n",
    "\n",
    "# Start timing\n",
    "print(\"Starting computation...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Compute results\n",
    "results = files_bag.map(process_station).compute()\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"Computation complete!\")\n",
    "print(f\"Processed {len(results)} stations in {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab9e391-d881-42a3-92e7-13cb64138044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Step 4: Close Clusters\n",
    "client.close()\n",
    "local_cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
